{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import *\n",
    "from pyspark.sql.functions import *\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Spark will automatically use the master specified in spark-defaults.conf\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfconfig=pd.read_csv('/home/jovyan/Notebooks/configpath.csv')\n",
    "dfpath=spark.createDataFrame(dfconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_path_processed = dfpath.filter(col(\"DataFeedName\") == \"Fact_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(col(\"DataFeedName\") == \"Fact_CSV_Path\").select('Path').collect()[0][0]\n",
    "source_path = dfpath.filter(col(\"DataFeedName\") == \"FactFile\").select('Path').collect()[0][0]\n",
    "sheet_name =dfpath.filter(col(\"DataFeedName\") == \"sheet_name\").select('Path').collect()[0][0]\n",
    "calendar_path =dfpath.filter(col(\"DataFeedName\") == \"Calendar_Delta_Path\").select('Path').collect()[0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pandas_df = pd.read_excel(source_path, sheet_name = sheet_name)\n",
    "df=spark.createDataFrame(pandas_df)\n",
    "df= df.withColumn(\"Date\", date_format(df[\"Date\"], \"yyyy-MM-dd\").cast(\"date\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    dffact = spark.sql(f\"SELECT MAX(datesk) as max_date FROM delta.`{trgt_path_processed}`\")\n",
    "    # Collect the result\n",
    "    max_date = dffact.withColumn(\"max_date\",\n",
    "                                    concat_ws(\"-\",col(\"max_date\").substr(1,4),col(\"max_date\").substr(5,2),col(\"max_date\").substr(7,2))\n",
    "                                    .cast(\"date\")).collect()[0][\"max_date\"]\n",
    "    query=f\"SELECT * FROM vw_src WHERE Date >= '{max_date}'\"\n",
    "    \n",
    "else:\n",
    "    query=\"SELECT * FROM vw_src\"\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.createOrReplaceTempView(\"vw_src\")\n",
    "df_src = spark.sql(query)\n",
    "df_src.show()\n",
    "print(df_src.count())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp =(\n",
    "df_src\n",
    "    .withColumnRenamed(\"Spending Item\", \"SpendingItem\")\n",
    "    .withColumnRenamed(\"Spend Amount\", \"SpendAmount\")\n",
    "    .withColumnRenamed(\"Wallet used\", \"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_output =(\n",
    "    df_temp\n",
    "    .withColumn(\"WalletSK\", xxhash64(\"WalletUsed\"))\n",
    "    .withColumn(\"categorysk\", xxhash64(\"category\").cast(\"long\"))\n",
    "    .withColumn(\"DateSK\", regexp_replace(\"date\", \"-\", \"\").cast(\"string\"))\n",
    "    .withColumn(\"PKSK\", xxhash64(concat(\"category\", \"WalletUsed\", \"Index\", \"Date\")).cast(\"string\"))\n",
    "    .withColumn(\"UpdateTimeStamp\", date_format(current_timestamp(), \"yyyy-MM-dd HH:mm:ss\").cast(\"timestamp\"))\n",
    "    .withColumn(\"RowSK\", xxhash64(concat_ws(\"|\", *[col(c) for c in df_temp.columns])))\n",
    "    .drop(\"Index\", \"Date\", \"category\",\"WalletUsed\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_output.createOrReplaceTempView(\"vw_source\")\n",
    "\n",
    "duplicate_counts = spark.sql(\"\"\"\n",
    "    SELECT COUNT(PKSK) as count\n",
    "    FROM vw_source\n",
    "    GROUP BY PKSK\n",
    "    HAVING COUNT(PKSK) > 1\n",
    "\"\"\")\n",
    "x = [row['count'] for row in duplicate_counts.collect()]\n",
    "\n",
    "# Fail the code if there are duplicates\n",
    "if len(x) > 0:\n",
    "    raise ValueError(f\"Duplicate values found :\\n{duplicate_counts}\")\n",
    "else:\n",
    "# # Proceed with the rest of the code if no duplicates\n",
    "    print(\"No duplicates found. Continuing execution...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    column_name = df_output.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause=\",\".join(column_name)\n",
    "    insert_values=\",\".join([f\"source.{i}\" for i in column_name])\n",
    "    query = f\"\"\"MERGE INTO delta.`{trgt_path_processed}` AS target \n",
    "            USING vw_source AS source \n",
    "            ON target.PKSK = source.PKSK \n",
    "            AND target.RowSK <> source.RowSK \n",
    "            WHEN MATCHED THEN UPDATE SET {set_clause}\n",
    "            WHEN NOT MATCHED THEN INSERT ({insert_clause}) VALUES ({insert_values})\"\"\"\n",
    "    spark.sql(query)        \n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "    spark.sql(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1).write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.format(\"csv\").option(\"header\",\"true\").load(trgt_path_csv).count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"SELECT UpdateTimeStamp FROM delta.`{trgt_path_processed}` \n",
    "          WHERE UpdateTimeStamp = (SELECT MAX(UpdateTimeStamp) FROM vw_source)\"\"\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
