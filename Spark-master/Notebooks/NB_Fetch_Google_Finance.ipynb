{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b657a786",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yfinance as yf\n",
    "import datetime as dt\n",
    "import requests, json, pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from pyspark.sql import *\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import DoubleType\n",
    "import pandas as pd\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "PYSPARK_PYTHON = os.getenv(\"PYSPARK_PYTHON\") \n",
    "PYSPARK_DRIVER_PYTHON = os.getenv(\"PYSPARK_DRIVER_PYTHON\")\n",
    "import pyspark\n",
    "from delta import configure_spark_with_delta_pip, DeltaTable\n",
    "import json\n",
    "\n",
    "# Load the configuration JSON file\n",
    "with open('/usr/local/spark/conf/spark-defaults.json', 'r') as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "# Initialize the Spark session builder\n",
    "builder = pyspark.sql.SparkSession.builder.appName(\"MyApp1\").config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\").config(\"spark.pyspark.python\", PYSPARK_PYTHON)\\\n",
    "    .config(\"spark.pyspark.driver.python\", PYSPARK_DRIVER_PYTHON)\n",
    "\n",
    "# Read the packages from the text file\n",
    "packages = []\n",
    "with open('/usr/local/spark/conf/packages.txt', 'r') as file:\n",
    "    # Read each line and strip newlines or extra spaces\n",
    "    packages = [line.strip() for line in file if line.strip()]\n",
    "\n",
    "# # Add packages to the Spark session configuration\n",
    "builder.config(\"spark.jars.packages\", \",\".join(packages))\n",
    "\n",
    "# Apply the configurations from the JSON file to the Spark session\n",
    "for key, value in config.items():\n",
    "    builder.config(key, value)\n",
    "\n",
    "# Configure Spark with Delta Lake (if needed)\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "# Now you can use the Spark session\n",
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "58e45228",
   "metadata": {},
   "outputs": [],
   "source": [
    "dfpd=pd.read_csv('/app/Notebooks/Config_Stock.csv')\n",
    "dfpath=spark.createDataFrame(dfpd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c43a6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "trgt_path_processed = dfpath.filter(F.col(\"DataFeedName\") == \"Stock_Delta_Path\").select('Path').collect()[0][0]\n",
    "trgt_path_csv = dfpath.filter(F.col(\"DataFeedName\") == \"Stock_CSV_Path\").select('Path').collect()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cb50d958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_roe(financials_info, balance_sheet):\n",
    "    net_income = financials_info.loc[\"Net Income\"].iloc[0] if \"Net Income\" in financials_info.index else None\n",
    "    # Fetch Shareholders' Equity from balance sheet\n",
    "    total_equity = balance_sheet.loc[\"Stockholders Equity\"].iloc[0] if \"Stockholders Equity\" in balance_sheet.index else None \n",
    "    return (net_income / total_equity) * 100 if type(net_income) == float and type(total_equity) == float else 0.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bb40760",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_roce(financials_info, balance_sheet):\n",
    "    ebit = financials_info.loc[\"Operating Income\"].iloc[0] if \"Operating Income\" in financials_info.index else 0\n",
    "\n",
    "    # # Get Total Assets and Current Liabilities from balance sheet\n",
    "    total_assets = balance_sheet.loc[\"Total Assets\"].iloc[0] if \"Total Assets\" in balance_sheet.index else 0\n",
    "    current_liabilities = balance_sheet.loc[\"Current Liabilities\"].iloc[0] if \"Current Liabilities\" in balance_sheet.index else 0\n",
    "\n",
    "    # Calculate Capital Employed\n",
    "    capital_employed = total_assets - current_liabilities\n",
    "\n",
    "    return (ebit / capital_employed) * 100 if capital_employed != 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f24f08bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_PEG(stock_info):\n",
    "# Calculate PEG ratios\n",
    "    trailing_pe = stock_info.get(\"trailingPE\", None)\n",
    "    forward_pe = stock_info.get(\"forwardPE\", None)\n",
    "    earnings_growth = stock_info.get(\"earningsGrowth\", None)  # Provided as a decimal\n",
    "\n",
    "    if earnings_growth is not None and earnings_growth > 0:\n",
    "        \n",
    "        trailing_peg = trailing_pe / (earnings_growth * 100) if trailing_pe else 0\n",
    "        forward_peg = forward_pe / (earnings_growth * 100) if forward_pe else 0\n",
    "        peg_t= trailing_peg if trailing_peg else \"N/A\"\n",
    "        peg_f=forward_peg if forward_peg else \"N/A\"\n",
    "    else:\n",
    "        peg_f=peg_t=\"N/A\"\n",
    "    return peg_t,peg_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11056faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_debt_to_equity(balance_sheet):\n",
    "    total_liabilities = balance_sheet.loc[\"Total Liabilities Net Minority Interest\"].iloc[0] if \"Total Liabilities Net Minority Interest\" in balance_sheet.index else 0\n",
    "    shareholders_equity = balance_sheet.loc[\"Stockholders Equity\"].iloc[0] if \"Stockholders Equity\" in balance_sheet.index else 0\n",
    "    # Calculate Debt-to-Equity Ratio\n",
    "    if shareholders_equity != 0:  # Avoid division by zero\n",
    "        debt_to_equity_ratio = total_liabilities / shareholders_equity\n",
    "    else:\n",
    "        debt_to_equity_ratio = \"N/A\"\n",
    "    return debt_to_equity_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9c438c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_sales_growth(income_statement):\n",
    "    revenue = income_statement.loc[\"Total Revenue\"] if \"Total Revenue\" in income_statement.index else {\"0\":\"NA\"}\n",
    "    revenue = revenue.dropna() if isinstance(revenue, pd.Series) else revenue # Remove any periods with missing data\n",
    "    # Ensure revenue has at least two periods to calculate growth\n",
    "    if len(revenue) > 1:\n",
    "        # Calculate sales growth between the latest two periods\n",
    "        latest_growth = ((revenue.iloc[0] - revenue.iloc[1]) / revenue.iloc[1]) * 100 if revenue.iloc[1] != 0 else 0\n",
    "        latest_period = revenue.index[0].strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        # Handle cases where there isn't enough data\n",
    "        latest_growth=0\n",
    "        latest_period=0\n",
    "    return latest_growth,latest_period"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "48b092e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_MA(historical_data):\n",
    "    # Calculate 50-day and 200-day moving averages\n",
    "    if not historical_data.empty:\n",
    "        historical_data[\"MA50\"] = historical_data[\"Close\"].rolling(window=50).mean()\n",
    "        historical_data[\"MA200\"] = historical_data[\"Close\"].rolling(window=200).mean()\n",
    "        # Return the latest MA50 and MA200\n",
    "        latest_data = historical_data.iloc[-1]\n",
    "        ma50=latest_data[\"MA50\"] if latest_data[\"MA50\"] else 0\n",
    "        ma200=latest_data[\"MA200\"] if latest_data[\"MA200\"] else 0\n",
    "    else:\n",
    "        # Return the latest MA50 and MA200\n",
    "        latest_data = 0\n",
    "        ma50=0\n",
    "        ma200=0\n",
    "    return ma50, ma200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "08cfd16d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def f_stock_data(l_tickers):\n",
    "    ticker_data = []\n",
    "    headers=[\"Ticker\",\"Sector\",\"Industry\",\"fiftytwo_week_high\",\"ROE\",\"ROCE\",\"Trailing_PEG\",\"Forward_PEG\",\"Debt_to_Equity\",\"Latest_Finanacial_Year\",\"Sales_Growth\",\"MA50\",\"MA200\"]\n",
    "    for t in l_tickers:\n",
    "        ticker = yf.Ticker(t + ('.BO' if t.isdigit() else '.NS'))\n",
    "        stock_info = ticker.info\n",
    "        balance_sheet = ticker.balance_sheet\n",
    "        financials_info=ticker.financials\n",
    "        income_statement=ticker.income_stmt\n",
    "        historical_data = ticker.history(period=\"ytd\")\n",
    "        v_roe=f_roe(financials_info, balance_sheet)\n",
    "        v_roce=f_roce(financials_info, balance_sheet)\n",
    "        v_peg_t,v_peg_f=f_PEG(stock_info)\n",
    "        v_debt_to_equity=f_debt_to_equity(balance_sheet)\n",
    "        v_sales_growth,v_latest_period=f_sales_growth(income_statement)\n",
    "        v_ma50,v_ma200=f_MA(historical_data)\n",
    "        ticker_data.append([t, stock_info.get(\"sector\", \"N/A\"), stock_info.get(\"industry\", \"N/A\"), stock_info.get(\"fiftyTwoWeekHigh\", None),v_roe,v_roce,v_peg_t,v_peg_f,v_debt_to_equity,v_latest_period,v_sales_growth,v_ma50,v_ma200])\n",
    "    df_retun = pd.DataFrame(ticker_data, columns=headers)\n",
    "    return(df_retun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d29cafcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pd=pd.read_csv(\"/home/jovyan/Notebooks/Holiday.csv\")\n",
    "df_holday_csv=spark.createDataFrame(df_pd)\n",
    "holiday=\"('\" + \"','\".join([row[\"Date\"] for row in df_holday_csv.select('Date').collect()]) + \"')\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd8a84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#To prevent Data overwriting on Market Hours, Preventing the Auto run of this notebook on NSE and BSE timings (9 AM to 4 PM IST)\n",
    "# Get the current date and time\n",
    "# current_timestamp = dt.datetime.now()\n",
    "# current_day=current_timestamp.strftime(\"%d-%b-%Y\")\n",
    "# week_day=current_timestamp.strftime('%A')\n",
    "# # Replace the time portion with 03:30:00 and 10:30:00\n",
    "# final_timestamp = current_timestamp.replace(hour=3, minute=30, second=0, microsecond=0)\n",
    "# end_timestamp = current_timestamp.replace(hour=10, minute=30, second=0, microsecond=0)\n",
    "\n",
    "# print(current_timestamp, current_day, week_day)\n",
    "\n",
    "# if current_day in holiday:\n",
    "#     print(f\"Today {current_day} is a Holiday of {df_holday_csv.filter(F.col('Date') == current_day).select('Description').collect()[0][0]}\\nExiting...\")\n",
    "#     exit()\n",
    "# elif week_day in ['Saturday','Sunday']:\n",
    "#     print(f\"Today is {week_day}\\nExiting...\")\n",
    "#     exit()\n",
    "# elif final_timestamp <= current_timestamp <= end_timestamp:\n",
    "#     print(f\"The current time {current_timestamp.time()} is between the start {final_timestamp.time()} and end {end_timestamp.time()} timestamps. Market is LIVE NOW \\nExiting...\")\n",
    "#     exit()\n",
    "# elif current_timestamp < final_timestamp:\n",
    "#     v_yesterday=v_yesterday=(current_timestamp - dt.timedelta(days=1)).date().strftime(\"%d-%b-%Y\")\n",
    "#     print(v_yesterday)\n",
    "#     if v_yesterday in holiday:\n",
    "#         print(f\"Yesterday {current_timestamp.date()} was Holiday of {df_holday_csv.filter(F.col('Date') == current_day).select('Description').collect()[0][0]}\")\n",
    "#         exit()\n",
    "#     elif v_yesterday in ['Saturday','Sunday']:\n",
    "#         print(f\"Yesterday is {week_day}\\nExiting...\")\n",
    "#         exit()\n",
    "#     else:\n",
    "#         print(f\"The current time {current_timestamp.time()} is NOT between the start {final_timestamp.time()} and end {end_timestamp.time()} timestamps. Market is CLOSED NOW\\nContinuing Execution...\")\n",
    "# else:\n",
    "#     print(f\"The current time {current_timestamp.time()} is NOT between the start {final_timestamp.time()} and end {end_timestamp.time()} timestamps. Market is CLOSED NOW\\nContinuing Execution...\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "941f616d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# v_yesterday=(current_timestamp - dt.timedelta(days=1)).date().strftime(\"%d-%b-%Y\")\n",
    "# v_day=v_yesterday\n",
    "# print(v_yesterday)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ee90d349",
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to scrape\n",
    "url_link=[\"https://www.google.com/finance/markets/gainers\",\"https://www.google.com/finance/markets/losers\"]\n",
    "rows = []\n",
    "headers=[\"Ticker\",\"Stock_Name\",\"CMP\",\"Change\",\"Change_Percentage\"]\n",
    "\n",
    "for url in url_link:\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        # Find the parent container\n",
    "        parent_container = soup.find('ul', class_='sbnBtf')\n",
    "        if parent_container:\n",
    "            # Find all stock entries within the parent container\n",
    "            stock_entries = parent_container.find_all('li')\n",
    "            for stock in stock_entries:\n",
    "                # Extract relevant details for each stock\n",
    "                stock_ticker = stock.find('div', class_='COaKTb').text if stock.find('div', class_='COaKTb') else \"N/A\"\n",
    "                stock_name = stock.find('div', class_='ZvmM7').text if stock.find('div', class_='ZvmM7') else \"N/A\"\n",
    "                stock_price = stock.find('div', class_='YMlKec').text if stock.find('div', class_='YMlKec') else \"N/A\"\n",
    "                stock_change = stock.find('div', class_='BAftM').text if stock.find('div', class_='BAftM') else \"N/A\"\n",
    "                stock_percent = stock.find('div', class_='zWwE1').text if stock.find('div', class_='zWwE1') else \"N/A\"\n",
    "                # Add extracted data to the list\n",
    "                rows.append([stock_ticker,stock_name,stock_price,stock_change,stock_percent])\n",
    "    \n",
    "    # Convert to JSON string with readable characters\n",
    "df_pd_today = pd.DataFrame(rows, columns=headers)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b8e34be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract unique tickers as a Python list\n",
    "l_tickers = df_pd_today[\"Ticker\"].unique().tolist()\n",
    "# Print the result\n",
    "print(l_tickers)\n",
    "df_stock_data=f_stock_data(l_tickers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1cd9b46b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_custom=df_pd_today.merge(df_stock_data, on='Ticker',how='left')\n",
    "reorder_colms=[\"Ticker\",\"Stock_Name\",\"Sector\",\"Industry\",\"CMP\",\"Change\",\"Change_Percentage\"]+[col for col in df_custom.columns if col not in [\"Ticker\",\"Stock_Name\",\"Sector\",\"Industry\",\"CMP\",\"Change\",\"Change_Percentage\"]]\n",
    "df_spark=spark.createDataFrame(df_custom[reorder_colms])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fe22719c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_master = df_spark.filter(\n",
    "                            (F.col(\"ROE\") >= 15) &\n",
    "                            (F.col(\"ROCE\") >= 15) &\n",
    "                            (F.col(\"Debt_to_Equity\") <= 1) &\n",
    "                            (F.col(\"MA50\") >= F.col(\"MA200\"))\n",
    "                            ).withColumn(\"ROE\",F.coalesce(F.round(F.col(\"ROE\"), 2),F.lit(0))) \\\n",
    "                            .withColumn(\"ROCE\", F.coalesce(F.round(F.col(\"ROCE\"), 2),F.lit(0))) \\\n",
    "                            .withColumn(\"Trailing_PEG\", F.coalesce(F.round(F.col(\"Trailing_PEG\"), 2),F.lit(0))) \\\n",
    "                            .withColumn(\"Forward_PEG\", F.coalesce(F.round(F.col(\"Forward_PEG\"), 2),F.lit(0))) \\\n",
    "                            .withColumn(\"Debt_to_Equity\", F.coalesce(F.round(F.col(\"Debt_to_Equity\"), 2),F.lit(0))) \\\n",
    "                            .withColumn(\"Sales_Growth\", F.coalesce(F.round(F.col(\"Sales_Growth\"), 2), F.lit(0))) \\\n",
    "                            .withColumn(\"MA50\", F.coalesce(F.round(F.col(\"MA50\"), 2), F.lit(0))) \\\n",
    "                            .withColumn(\"MA200\", F.coalesce(F.round(F.col(\"MA200\"), 2), F.lit(0))) \\\n",
    "                            .withColumn(\n",
    "                                \"Gainer_Looser\",\n",
    "                                F.when(\n",
    "                                    F.regexp_replace(F.col(\"Change\"), \"â‚¹\", \"\").cast(\"float\") < 0.0, \"L\"\n",
    "                                ).otherwise(\"G\")\n",
    "                            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "a546ecbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    df_read = spark.read.format('delta').load(trgt_path_processed)\n",
    "    df_repeat=df_read.join(df_master.select('Ticker'), on='Ticker', how='inner')\\\n",
    "            .withColumn('RunTimeStamp',(F.current_timestamp()).cast(\"timestamp\"))\\\n",
    "            .withColumn(\"NextDay\", F.to_timestamp(F.date_format(F.date_add(F.col(\"UpdateTimestamp\"), 1), format=\"yyyy-MM-dd 10:30:00\")))\\\n",
    "            .withColumn(\"WatchOutFlag\", F.when(\n",
    "                F.col('RunTimeStamp') > F.col('NextDay'), (F.col('WatchOutFlag') + 1)).otherwise(F.col('WatchOutFlag'))) \\\n",
    "            .withColumn(\"UpdateTimestamp\",  F.when(\n",
    "                        F.col('RunTimeStamp') > F.col('NextDay'),\n",
    "                        F.to_timestamp(F.date_format(F.current_date(), format=\"yyyy-MM-dd 10:30:00\"))).otherwise(F.col('UpdateTimestamp')))\\\n",
    "            .drop(\"RunTimeStamp\",\"NextDay\",\"PKSK\",\"RowSK\")\n",
    "    replace_tickers = df_repeat.select(\"Ticker\").rdd.flatMap(lambda x: x).collect()\n",
    "    df_output = (df_master.filter(~F.col(\"Ticker\").isin(replace_tickers))).unionByName(df_repeat, allowMissingColumns=True) \\\n",
    "        .withColumn(\"WatchOutFlag\", F.coalesce(F.col(\"WatchOutFlag\"), F.lit(0)).cast('int'))\\\n",
    "        .withColumn('UpdateTimestamp', F.coalesce(F.col(\"UpdateTimestamp\"),F.date_format(F.current_timestamp(), format=\"yyyy-MM-dd 10:30:00\")))   \n",
    "else:\n",
    "    df_output=df_master.withColumn('WatchOutFlag',F.lit(0))\\\n",
    "                .withColumn('UpdateTimestamp', F.date_format(F.current_timestamp(), format=\"yyyy-MM-dd 10:30:00\"))\n",
    "df_final=df_output.withColumn(\"PKSK\", F.xxhash64(F.col(\"Ticker\")).cast(\"string\"))\\\n",
    "        .withColumn(\"RowSK\", F.xxhash64(F.concat_ws(\"|\", *[F.col(c) for c in df_output.columns])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "0122d429",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.createOrReplaceTempView('vw_source')\n",
    "dup_query=\"select PKSK from vw_source group by 1 having count(PKSK)>1 \"\n",
    "df_dup=spark.sql(dup_query)\n",
    "x = df_dup.count()\n",
    "if x == 0:\n",
    "    print(\"No duplicates.. continuing execution\")\n",
    "else:\n",
    "    print(f\"Exception of duplicates! Found {x} duplicate keys.\")\n",
    "    # #exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "e6039500",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DeltaTable.isDeltaTable(spark, trgt_path_processed):\n",
    "    # First delete records where both PKSK and RowSK match\n",
    "    deletion_query = f\"\"\"\n",
    "    MERGE INTO delta.`{trgt_path_processed}` AS target\n",
    "    USING vw_source AS source\n",
    "    ON target.PKSK = source.PKSK AND target.RowSK = source.RowSK\n",
    "    WHEN MATCHED THEN DELETE\n",
    "    \"\"\"\n",
    "    spark.sql(deletion_query)\n",
    "    \n",
    "    # Then handle updates (different RowSK) and inserts\n",
    "    column_name = df_final.columns\n",
    "    set_clause = \", \".join([f\"target.{i} = source.{i}\" for i in column_name])\n",
    "    insert_clause = \",\".join(column_name)\n",
    "    insert_values = \",\".join([f\"source.{i}\" for i in column_name])\n",
    "    \n",
    "    merge_query = f\"\"\"\n",
    "    MERGE INTO delta.`{trgt_path_processed}` AS target\n",
    "    USING vw_source AS source\n",
    "    ON target.PKSK = source.PKSK\n",
    "    WHEN MATCHED AND target.RowSK <> source.RowSK THEN\n",
    "        UPDATE SET {set_clause}\n",
    "    WHEN NOT MATCHED THEN\n",
    "        INSERT ({insert_clause}) VALUES ({insert_values})\n",
    "    \"\"\"\n",
    "    spark.sql(merge_query)     \n",
    "else :\n",
    "    query=f\"CREATE TABLE delta.`{trgt_path_processed}` USING DELTA AS SELECT * FROM vw_source\"\n",
    "    spark.sql(query)\n",
    "\n",
    "print(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "953f786a",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", False)\n",
    "delta_table=DeltaTable.forPath(spark, trgt_path_processed)\n",
    "delta_table.vacuum(retentionHours=0)\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "38061e5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_read = spark.read.format('delta').load(trgt_path_processed)\n",
    "display(df_read)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "d5090ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "spark.read.format(\"delta\").load(trgt_path_processed).coalesce(1)\\\n",
    "    .write.format(\"csv\").option(\"header\",\"true\").mode(\"overwrite\").save(trgt_path_csv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "15f03a61",
   "metadata": {},
   "outputs": [],
   "source": [
    "trgt_copy_path = trgt_path_csv + \"processed.csv\"\n",
    "files=os.listdir(trgt_path_csv)\n",
    "selected_files = [file for file in files if file.startswith('part-00') and file.endswith('.csv')]\n",
    "file=trgt_path_csv + selected_files[0]\n",
    "print(selected_files)\n",
    "shutil.copy(file, trgt_copy_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "70ad0d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_log = [file for file in files if \"processed.csv\" != file ]\n",
    "for file in delete_log :\n",
    "    os.remove(trgt_path_csv + file)\n",
    "    print(f\"removed {trgt_path_csv + file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
